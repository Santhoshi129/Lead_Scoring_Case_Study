{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Lead Scoring Case Study\n\n__Problem Statement :__\n    An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\n    The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\n    There are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n    X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.","metadata":{}},{"cell_type":"markdown","source":"__Our Goals of the Case Study:__\n\n- To **build a logistic regression model to assign a lead score** between 0 and 100 to each of the leads which can be used by the company to target potential leads. \n- To **adjust to if the company's requirement changes** in the future so you will need to handle these as well.","metadata":{}},{"cell_type":"markdown","source":"__The steps are broadly:__\n1. Read and understand the data\n2. Clean the data\n3. Prepare the data for Model Building\n4. Model Building\n5. Model Evaluation\n6. Making Predictions on the Test Set","metadata":{}},{"cell_type":"markdown","source":"# Import modules","metadata":{}},{"cell_type":"code","source":"# Supress unnecessary warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport time, warnings\nimport datetime as dt\n\nfrom IPython.display import display\npd.options.display.max_columns = None\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:30.879434Z","iopub.execute_input":"2024-05-19T12:49:30.879783Z","iopub.status.idle":"2024-05-19T12:49:32.146799Z","shell.execute_reply.started":"2024-05-19T12:49:30.879754Z","shell.execute_reply":"2024-05-19T12:49:32.145874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read and understand the data","metadata":{}},{"cell_type":"code","source":"xleads = pd.read_csv(r'C:\\Users\\indranil1\\Desktop\\Indranil-Personal\\Case Study\\Machine Learning\\Lead+Scoring+Case+Study\\Lead Scoring Assignment\\Leads.csv')\n\n# Look at the first few entries\nxleads.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.151762Z","iopub.execute_input":"2024-05-19T12:49:32.154144Z","iopub.status.idle":"2024-05-19T12:49:32.961863Z","shell.execute_reply.started":"2024-05-19T12:49:32.154100Z","shell.execute_reply":"2024-05-19T12:49:32.954995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect the shape of the dataset\n\nxleads.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.966361Z","iopub.status.idle":"2024-05-19T12:49:32.969110Z","shell.execute_reply.started":"2024-05-19T12:49:32.968798Z","shell.execute_reply":"2024-05-19T12:49:32.968826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect the different columsn in the dataset\n\nxleads.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.973386Z","iopub.status.idle":"2024-05-19T12:49:32.975649Z","shell.execute_reply.started":"2024-05-19T12:49:32.975386Z","shell.execute_reply":"2024-05-19T12:49:32.975413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xleads.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.979698Z","iopub.status.idle":"2024-05-19T12:49:32.982056Z","shell.execute_reply.started":"2024-05-19T12:49:32.981794Z","shell.execute_reply":"2024-05-19T12:49:32.981821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the summary of the dataset\n\nxleads.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.983489Z","iopub.status.idle":"2024-05-19T12:49:32.988193Z","shell.execute_reply.started":"2024-05-19T12:49:32.987884Z","shell.execute_reply":"2024-05-19T12:49:32.987911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the info to see the types of the feature variables and the null values present\n\nxleads.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.989588Z","iopub.status.idle":"2024-05-19T12:49:32.990472Z","shell.execute_reply.started":"2024-05-19T12:49:32.990064Z","shell.execute_reply":"2024-05-19T12:49:32.990083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are quite a few categorical variables present in this dataset for which we will need to create dummy variables. Also, there are a lot of null values present as well, so we will need to treat them accordingly.","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning and Preparation","metadata":{}},{"cell_type":"code","source":"# Check the number of missing values in each column\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:32.992120Z","iopub.status.idle":"2024-05-19T12:49:32.992962Z","shell.execute_reply.started":"2024-05-19T12:49:32.992738Z","shell.execute_reply":"2024-05-19T12:49:32.992759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see there are a lot of column which have high number of missing values. Clearly, these columns are not useful. Since, there are 9000 datapoints in our dataframe, let's eliminate the columns having greater than 3000 missing values as they are of no use to us.","metadata":{}},{"cell_type":"code","source":"# Drop all the columns in which greater than 3000 missing values are present\n\nfor col in xleads.columns:\n    if xleads[col].isnull().sum() > 3000:\n        xleads.drop(col, 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.001503Z","iopub.status.idle":"2024-05-19T12:49:33.002327Z","shell.execute_reply.started":"2024-05-19T12:49:33.002086Z","shell.execute_reply":"2024-05-19T12:49:33.002107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.003565Z","iopub.status.idle":"2024-05-19T12:49:33.004341Z","shell.execute_reply.started":"2024-05-19T12:49:33.004113Z","shell.execute_reply":"2024-05-19T12:49:33.004134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you might be able to interpret, the variable City won't be of any use in our analysis. So it's best that we drop it.","metadata":{}},{"cell_type":"code","source":"xleads.drop(['City'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.005594Z","iopub.status.idle":"2024-05-19T12:49:33.006329Z","shell.execute_reply.started":"2024-05-19T12:49:33.006105Z","shell.execute_reply":"2024-05-19T12:49:33.006125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same goes for the variable 'Country'\n\nxleads.drop(['Country'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.007663Z","iopub.status.idle":"2024-05-19T12:49:33.008403Z","shell.execute_reply.started":"2024-05-19T12:49:33.008195Z","shell.execute_reply":"2024-05-19T12:49:33.008217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's now check the percentage of missing values in each column\n\nround(100*(xleads.isnull().sum()/len(xleads.index)), 2)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.009803Z","iopub.status.idle":"2024-05-19T12:49:33.010578Z","shell.execute_reply.started":"2024-05-19T12:49:33.010374Z","shell.execute_reply":"2024-05-19T12:49:33.010394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.019200Z","iopub.status.idle":"2024-05-19T12:49:33.020063Z","shell.execute_reply.started":"2024-05-19T12:49:33.019829Z","shell.execute_reply":"2024-05-19T12:49:33.019854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now recall that there are a few columns in which there is a level called 'Select' which basically means that the student had not selected the option for that particular column which is why it shows 'Select'. These values are as good as missing values and hence we need to identify the value counts of the level 'Select' in all the columns that it is present.","metadata":{}},{"cell_type":"code","source":"# Get the value counts of all the columns\n\nfor column in xleads:\n    print(xleads[column].astype('category').value_counts())\n    print('___________________________________________________')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.021487Z","iopub.status.idle":"2024-05-19T12:49:33.022189Z","shell.execute_reply.started":"2024-05-19T12:49:33.021970Z","shell.execute_reply":"2024-05-19T12:49:33.021990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following three columns now have the level 'Select'. Let's check them once again.","metadata":{}},{"cell_type":"code","source":"xleads['Lead Profile'].astype('category').value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.023454Z","iopub.status.idle":"2024-05-19T12:49:33.024249Z","shell.execute_reply.started":"2024-05-19T12:49:33.024023Z","shell.execute_reply":"2024-05-19T12:49:33.024043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xleads['How did you hear about X Education'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.025600Z","iopub.status.idle":"2024-05-19T12:49:33.026350Z","shell.execute_reply.started":"2024-05-19T12:49:33.026128Z","shell.execute_reply":"2024-05-19T12:49:33.026162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xleads['Specialization'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.027735Z","iopub.status.idle":"2024-05-19T12:49:33.028767Z","shell.execute_reply.started":"2024-05-19T12:49:33.028478Z","shell.execute_reply":"2024-05-19T12:49:33.028504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly the levels Lead Profile and How did you hear about X Education have a lot of rows which have the value Select which is of no use to the analysis so it's best that we drop them.","metadata":{}},{"cell_type":"code","source":"xleads.drop(['Lead Profile', 'How did you hear about X Education'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.030140Z","iopub.status.idle":"2024-05-19T12:49:33.037633Z","shell.execute_reply.started":"2024-05-19T12:49:33.037323Z","shell.execute_reply":"2024-05-19T12:49:33.037354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also notice that when you got the value counts of all the columns, there were a few columns in which only one value was majorly present for all the data points. These include `Do Not Call`, `Search`, `Magazine`, `Newspaper Article`, `X Education Forums`, `Newspaper`, `Digital Advertisement`, `Through Recommendations`, `Receive More Updates About Our Courses`, `Update me on Supply Chain Content`, `Get updates on DM Content`, `I agree to pay the amount through cheque`. Since practically all of the values for these variables are `No`, it's best that we drop these columns as they won't help with our analysis.","metadata":{}},{"cell_type":"code","source":"xleads.drop(['Do Not Call', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', \n            'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', \n            'Update me on Supply Chain Content', 'Get updates on DM Content', \n            'I agree to pay the amount through cheque'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.039446Z","iopub.status.idle":"2024-05-19T12:49:33.040335Z","shell.execute_reply.started":"2024-05-19T12:49:33.040063Z","shell.execute_reply":"2024-05-19T12:49:33.040087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, the variable `What matters most to you in choosing a course` has the level `Better Career Prospects` `6528` times while the other two levels appear once twice and once respectively. So we should drop this column as well.","metadata":{}},{"cell_type":"code","source":"xleads['What matters most to you in choosing a course'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.041717Z","iopub.status.idle":"2024-05-19T12:49:33.042498Z","shell.execute_reply.started":"2024-05-19T12:49:33.042276Z","shell.execute_reply":"2024-05-19T12:49:33.042297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the null value rows present in the variable 'What matters most to you in choosing a course'\n\nxleads.drop(['What matters most to you in choosing a course'], axis = 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.043882Z","iopub.status.idle":"2024-05-19T12:49:33.044665Z","shell.execute_reply.started":"2024-05-19T12:49:33.044464Z","shell.execute_reply":"2024-05-19T12:49:33.044483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.045955Z","iopub.status.idle":"2024-05-19T12:49:33.046681Z","shell.execute_reply.started":"2024-05-19T12:49:33.046478Z","shell.execute_reply":"2024-05-19T12:49:33.046498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, there's the column `What is your current occupation` which has a lot of null values. Now you can drop the entire row but since we have already lost so many feature variables, we choose not to drop it as it might turn out to be significant in the analysis. So let's just drop the null rows for the column `What is you current occupation`.","metadata":{}},{"cell_type":"code","source":"xleads = xleads[~pd.isnull(xleads['What is your current occupation'])]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.051178Z","iopub.status.idle":"2024-05-19T12:49:33.052069Z","shell.execute_reply.started":"2024-05-19T12:49:33.051826Z","shell.execute_reply":"2024-05-19T12:49:33.051856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.053504Z","iopub.status.idle":"2024-05-19T12:49:33.060602Z","shell.execute_reply.started":"2024-05-19T12:49:33.060302Z","shell.execute_reply":"2024-05-19T12:49:33.060339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since now the number of null values present in the columns are quite small we can simply drop the rows in which these null values are present.","metadata":{}},{"cell_type":"code","source":"# Drop the null value rows in the column 'TotalVisits'\n\nxleads = xleads[~pd.isnull(xleads['TotalVisits'])]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.062023Z","iopub.status.idle":"2024-05-19T12:49:33.062728Z","shell.execute_reply.started":"2024-05-19T12:49:33.062528Z","shell.execute_reply":"2024-05-19T12:49:33.062546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.063928Z","iopub.status.idle":"2024-05-19T12:49:33.064680Z","shell.execute_reply.started":"2024-05-19T12:49:33.064483Z","shell.execute_reply":"2024-05-19T12:49:33.064502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the null values rows in the column 'Lead Source'\n\nxleads = xleads[~pd.isnull(xleads['Lead Source'])]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.065963Z","iopub.status.idle":"2024-05-19T12:49:33.066661Z","shell.execute_reply.started":"2024-05-19T12:49:33.066467Z","shell.execute_reply":"2024-05-19T12:49:33.066485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.067914Z","iopub.status.idle":"2024-05-19T12:49:33.068650Z","shell.execute_reply.started":"2024-05-19T12:49:33.068453Z","shell.execute_reply":"2024-05-19T12:49:33.068471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the null values rows in the column 'Specialization'\n\nxleads = xleads[~pd.isnull(xleads['Specialization'])]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.069874Z","iopub.status.idle":"2024-05-19T12:49:33.077632Z","shell.execute_reply.started":"2024-05-19T12:49:33.077322Z","shell.execute_reply":"2024-05-19T12:49:33.077351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of null values again\n\nxleads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.079043Z","iopub.status.idle":"2024-05-19T12:49:33.079761Z","shell.execute_reply.started":"2024-05-19T12:49:33.079562Z","shell.execute_reply":"2024-05-19T12:49:33.079582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now your data doesn't have any null values. Let's now check the percentage of rows that we have retained.","metadata":{}},{"cell_type":"code","source":"print(len(xleads.index))\nprint(len(xleads.index)/9240)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.080996Z","iopub.status.idle":"2024-05-19T12:49:33.081657Z","shell.execute_reply.started":"2024-05-19T12:49:33.081463Z","shell.execute_reply":"2024-05-19T12:49:33.081482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still have around 69% of the rows which seems good enough.","metadata":{}},{"cell_type":"code","source":"# Let's look at the dataset again\n\nxleads.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.082972Z","iopub.status.idle":"2024-05-19T12:49:33.083705Z","shell.execute_reply.started":"2024-05-19T12:49:33.083502Z","shell.execute_reply":"2024-05-19T12:49:33.083521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, clearly the variables `Prospect ID` and `Lead Number` won't be of any use in the analysis, so it's best that we drop these two variables.","metadata":{}},{"cell_type":"code","source":"xleads.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.085035Z","iopub.status.idle":"2024-05-19T12:49:33.085753Z","shell.execute_reply.started":"2024-05-19T12:49:33.085559Z","shell.execute_reply":"2024-05-19T12:49:33.085578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xleads.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.086975Z","iopub.status.idle":"2024-05-19T12:49:33.087687Z","shell.execute_reply.started":"2024-05-19T12:49:33.087486Z","shell.execute_reply":"2024-05-19T12:49:33.087505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the data for modelling","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nsns.pairplot(xleads,diag_kind='kde',hue='Converted')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.095344Z","iopub.status.idle":"2024-05-19T12:49:33.096090Z","shell.execute_reply.started":"2024-05-19T12:49:33.095883Z","shell.execute_reply":"2024-05-19T12:49:33.095904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xedu = xleads[['TotalVisits','Total Time Spent on Website','Page Views Per Visit','Converted']]\nsns.pairplot(xedu,diag_kind='kde',hue='Converted')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.097296Z","iopub.status.idle":"2024-05-19T12:49:33.098003Z","shell.execute_reply.started":"2024-05-19T12:49:33.097809Z","shell.execute_reply":"2024-05-19T12:49:33.097828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\ntransformedxedu = pd.DataFrame(pt.fit_transform(xedu))\ntransformedxedu.columns = xedu.columns\ntransformedxedu.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.099295Z","iopub.status.idle":"2024-05-19T12:49:33.100001Z","shell.execute_reply.started":"2024-05-19T12:49:33.099804Z","shell.execute_reply":"2024-05-19T12:49:33.099823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(transformedxedu,diag_kind='kde',hue='Converted')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.101287Z","iopub.status.idle":"2024-05-19T12:49:33.101995Z","shell.execute_reply.started":"2024-05-19T12:49:33.101796Z","shell.execute_reply":"2024-05-19T12:49:33.101815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy variable creation\n\nThe next step is to deal with the categorical variables present in the dataset. So first take a look at which variables are actually categorical variables.","metadata":{}},{"cell_type":"code","source":"# Check the columns which are of type 'object'\n\ntemp = xleads.loc[:, xleads.dtypes == 'object']\ntemp.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.103238Z","iopub.status.idle":"2024-05-19T12:49:33.103923Z","shell.execute_reply.started":"2024-05-19T12:49:33.103729Z","shell.execute_reply":"2024-05-19T12:49:33.103747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dummy variables using the 'get_dummies' command\ndummy = pd.get_dummies(xleads[['Lead Origin', 'Lead Source', 'Do Not Email', 'Last Activity',\n                              'What is your current occupation','A free copy of Mastering The Interview', \n                              'Last Notable Activity']], drop_first=True)\n\n# Add the results to the master dataframe\nxleads = pd.concat([xleads, dummy], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.111656Z","iopub.status.idle":"2024-05-19T12:49:33.112454Z","shell.execute_reply.started":"2024-05-19T12:49:33.112219Z","shell.execute_reply":"2024-05-19T12:49:33.112242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dummy variable separately for the variable 'Specialization' since it has the level 'Select' which is useless so we\n# drop that level by specifying it explicitly\n\ndummy_spl = pd.get_dummies(xleads['Specialization'], prefix = 'Specialization')\ndummy_spl = dummy_spl.drop(['Specialization_Select'], 1)\nxleads = pd.concat([xleads, dummy_spl], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.113674Z","iopub.status.idle":"2024-05-19T12:49:33.114492Z","shell.execute_reply.started":"2024-05-19T12:49:33.114132Z","shell.execute_reply":"2024-05-19T12:49:33.114167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the variables for which the dummy variables have been created\n\nxleads = xleads.drop(['Lead Origin', 'Lead Source', 'Do Not Email', 'Last Activity',\n                   'Specialization', 'What is your current occupation',\n                   'A free copy of Mastering The Interview', 'Last Notable Activity'], 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.116093Z","iopub.status.idle":"2024-05-19T12:49:33.116890Z","shell.execute_reply.started":"2024-05-19T12:49:33.116677Z","shell.execute_reply":"2024-05-19T12:49:33.116698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at the dataset again\n\nxleads.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.118265Z","iopub.status.idle":"2024-05-19T12:49:33.118996Z","shell.execute_reply.started":"2024-05-19T12:49:33.118794Z","shell.execute_reply":"2024-05-19T12:49:33.118812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test-Train Split\n\nThe next step is to split the dataset into training an testing sets.","metadata":{}},{"cell_type":"code","source":"# Import the required library\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.120521Z","iopub.status.idle":"2024-05-19T12:49:33.122837Z","shell.execute_reply.started":"2024-05-19T12:49:33.122595Z","shell.execute_reply":"2024-05-19T12:49:33.122618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put all the feature variables in X\n\nX = xleads.drop(['Converted'], 1)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.131597Z","iopub.status.idle":"2024-05-19T12:49:33.132385Z","shell.execute_reply.started":"2024-05-19T12:49:33.132138Z","shell.execute_reply":"2024-05-19T12:49:33.132175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put the target variable in y\n\ny = xleads['Converted']\n\ny.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.133582Z","iopub.status.idle":"2024-05-19T12:49:33.134249Z","shell.execute_reply.started":"2024-05-19T12:49:33.134020Z","shell.execute_reply":"2024-05-19T12:49:33.134039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into 70% train and 30% test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.135567Z","iopub.status.idle":"2024-05-19T12:49:33.136284Z","shell.execute_reply.started":"2024-05-19T12:49:33.136069Z","shell.execute_reply":"2024-05-19T12:49:33.136088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling\n\nNow there are a few numeric variables present in the dataset which have different scales. So let's go ahead and scale these variables.","metadata":{}},{"cell_type":"code","source":"# Import MinMax scaler\n\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.137582Z","iopub.status.idle":"2024-05-19T12:49:33.138286Z","shell.execute_reply.started":"2024-05-19T12:49:33.138070Z","shell.execute_reply":"2024-05-19T12:49:33.138090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the three numeric features present in the dataset\n\nscaler = MinMaxScaler()\n\nX_train[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']] = scaler.fit_transform(X_train[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']])\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.139546Z","iopub.status.idle":"2024-05-19T12:49:33.140243Z","shell.execute_reply.started":"2024-05-19T12:49:33.140035Z","shell.execute_reply":"2024-05-19T12:49:33.140053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at the correlations\n\nLet's now look at the correlations. Since the number of variables are pretty high, it's better that we look at the table instead of plotting a heatmap","metadata":{}},{"cell_type":"code","source":"# Looking at the correlation table\nplt.figure(figsize = (25,15))\nsns.heatmap(xleads.corr())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.147746Z","iopub.status.idle":"2024-05-19T12:49:33.148615Z","shell.execute_reply.started":"2024-05-19T12:49:33.148375Z","shell.execute_reply":"2024-05-19T12:49:33.148400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\n\nLet's now move to model building. As you can see that there are a lot of variables present in the dataset which we cannot deal with. So the best way to approach this is to select a small set of features from this pool of variables using RFE.","metadata":{}},{"cell_type":"code","source":"# Import 'LogisticRegression' and create a LogisticRegression object\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.149968Z","iopub.status.idle":"2024-05-19T12:49:33.150666Z","shell.execute_reply.started":"2024-05-19T12:49:33.150464Z","shell.execute_reply":"2024-05-19T12:49:33.150483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import RFE and select 15 variables\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.151897Z","iopub.status.idle":"2024-05-19T12:49:33.152623Z","shell.execute_reply.started":"2024-05-19T12:49:33.152424Z","shell.execute_reply":"2024-05-19T12:49:33.152443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at which features have been selected by RFE\n\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.153893Z","iopub.status.idle":"2024-05-19T12:49:33.154598Z","shell.execute_reply.started":"2024-05-19T12:49:33.154397Z","shell.execute_reply":"2024-05-19T12:49:33.154416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put all the columns selected by RFE in the variable 'col'\n\ncol = X_train.columns[rfe.support_]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.155874Z","iopub.status.idle":"2024-05-19T12:49:33.156575Z","shell.execute_reply.started":"2024-05-19T12:49:33.156377Z","shell.execute_reply":"2024-05-19T12:49:33.156396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you have all the variables selected by RFE and since we care about the statistics part, i.e. the p-values and the VIFs, let's use these variables to create a logistic regression model using statsmodels.","metadata":{}},{"cell_type":"code","source":"# Select only the columns selected by RFE\n\nX_train = X_train[col]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.164138Z","iopub.status.idle":"2024-05-19T12:49:33.164950Z","shell.execute_reply.started":"2024-05-19T12:49:33.164730Z","shell.execute_reply":"2024-05-19T12:49:33.164752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import statsmodels\n\nimport statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.166190Z","iopub.status.idle":"2024-05-19T12:49:33.166833Z","shell.execute_reply.started":"2024-05-19T12:49:33.166644Z","shell.execute_reply":"2024-05-19T12:49:33.166663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit a logistic Regression model on X_train after adding a constant and output the summary\n\nX_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.168196Z","iopub.status.idle":"2024-05-19T12:49:33.168946Z","shell.execute_reply.started":"2024-05-19T12:49:33.168741Z","shell.execute_reply":"2024-05-19T12:49:33.168761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are quite a few variable which have a p-value greater than `0.05`. We will need to take care of them. But first, let's also look at the VIFs.","metadata":{}},{"cell_type":"code","source":"# Import 'variance_inflation_factor'\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.170258Z","iopub.status.idle":"2024-05-19T12:49:33.170962Z","shell.execute_reply.started":"2024-05-19T12:49:33.170764Z","shell.execute_reply":"2024-05-19T12:49:33.170782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.178924Z","iopub.status.idle":"2024-05-19T12:49:33.180059Z","shell.execute_reply.started":"2024-05-19T12:49:33.179810Z","shell.execute_reply":"2024-05-19T12:49:33.179836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"VIFs seem to be in a decent range except for three variables. \n\nLet's first drop the variable `Lead Source_Reference` since it has a high p-value as well as a high VIF.","metadata":{}},{"cell_type":"code","source":"X_train.drop('Lead Source_Reference', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.182288Z","iopub.status.idle":"2024-05-19T12:49:33.183520Z","shell.execute_reply.started":"2024-05-19T12:49:33.183234Z","shell.execute_reply":"2024-05-19T12:49:33.183257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.187141Z","iopub.status.idle":"2024-05-19T12:49:33.188240Z","shell.execute_reply.started":"2024-05-19T12:49:33.187984Z","shell.execute_reply":"2024-05-19T12:49:33.188006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The variable `Lead Profile_Dual Specialization Student\t` also needs to be dropped.","metadata":{}},{"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.199319Z","iopub.execute_input":"2024-05-19T12:49:33.199977Z","iopub.status.idle":"2024-05-19T12:49:33.262100Z","shell.execute_reply.started":"2024-05-19T12:49:33.199943Z","shell.execute_reply":"2024-05-19T12:49:33.260621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The VIFs are now all less than 5. So let's drop the ones with the high p-values beginning with `Last Notable Activity_Had a Phone Conversation`.","metadata":{}},{"cell_type":"code","source":"X_train.drop('Last Notable Activity_Had a Phone Conversation', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.262926Z","iopub.status.idle":"2024-05-19T12:49:33.263360Z","shell.execute_reply.started":"2024-05-19T12:49:33.263134Z","shell.execute_reply":"2024-05-19T12:49:33.263165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.265175Z","iopub.status.idle":"2024-05-19T12:49:33.265872Z","shell.execute_reply.started":"2024-05-19T12:49:33.265670Z","shell.execute_reply":"2024-05-19T12:49:33.265690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop `What is your current occupation_Housewife`.","metadata":{}},{"cell_type":"code","source":"X_train.drop('What is your current occupation_Housewife', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.282220Z","iopub.execute_input":"2024-05-19T12:49:33.282575Z","iopub.status.idle":"2024-05-19T12:49:33.326869Z","shell.execute_reply.started":"2024-05-19T12:49:33.282550Z","shell.execute_reply":"2024-05-19T12:49:33.325598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.327983Z","iopub.status.idle":"2024-05-19T12:49:33.328672Z","shell.execute_reply.started":"2024-05-19T12:49:33.328442Z","shell.execute_reply":"2024-05-19T12:49:33.328460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop `What is your current occupation_Working Professional`.","metadata":{}},{"cell_type":"code","source":"X_train.drop('What is your current occupation_Working Professional', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.349788Z","iopub.execute_input":"2024-05-19T12:49:33.350139Z","iopub.status.idle":"2024-05-19T12:49:33.380980Z","shell.execute_reply.started":"2024-05-19T12:49:33.350112Z","shell.execute_reply":"2024-05-19T12:49:33.377956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.384695Z","iopub.status.idle":"2024-05-19T12:49:33.386516Z","shell.execute_reply.started":"2024-05-19T12:49:33.386235Z","shell.execute_reply":"2024-05-19T12:49:33.386264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the p-values are now in the appropriate range. Let's also check the VIFs again in case we had missed something.","metadata":{}},{"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.421528Z","iopub.execute_input":"2024-05-19T12:49:33.423180Z","iopub.status.idle":"2024-05-19T12:49:33.454980Z","shell.execute_reply.started":"2024-05-19T12:49:33.423123Z","shell.execute_reply":"2024-05-19T12:49:33.452336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\n\nNow, both the p-values and VIFs seem decent enough for all the variables. So let's go ahead and make predictions using this final set of features.","metadata":{}},{"cell_type":"code","source":"# Use 'predict' to predict the probabilities on the train set\n\ny_train_pred = res.predict(sm.add_constant(X_train))\ny_train_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.482135Z","iopub.execute_input":"2024-05-19T12:49:33.483880Z","iopub.status.idle":"2024-05-19T12:49:33.512595Z","shell.execute_reply.started":"2024-05-19T12:49:33.483838Z","shell.execute_reply":"2024-05-19T12:49:33.509174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshaping it into an array\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.514654Z","iopub.status.idle":"2024-05-19T12:49:33.516245Z","shell.execute_reply.started":"2024-05-19T12:49:33.515965Z","shell.execute_reply":"2024-05-19T12:49:33.515989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a dataframe with the actual conversion flag and the predicted probabilities","metadata":{}},{"cell_type":"code","source":"# Create a new dataframe containing the actual conversion flag and the probabilities predicted by the model\n\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.564694Z","iopub.execute_input":"2024-05-19T12:49:33.566301Z","iopub.status.idle":"2024-05-19T12:49:33.594418Z","shell.execute_reply.started":"2024-05-19T12:49:33.566263Z","shell.execute_reply":"2024-05-19T12:49:33.591876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating new column 'Predicted' with 1 if Paid_Prob > 0.5 else 0","metadata":{}},{"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.621277Z","iopub.execute_input":"2024-05-19T12:49:33.622913Z","iopub.status.idle":"2024-05-19T12:49:33.652274Z","shell.execute_reply.started":"2024-05-19T12:49:33.622875Z","shell.execute_reply":"2024-05-19T12:49:33.649683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that you have the probabilities and have also made conversion predictions using them, it's time to evaluate the model.","metadata":{}},{"cell_type":"code","source":"# Import metrics from sklearn for evaluation\n\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:33.673600Z","iopub.execute_input":"2024-05-19T12:49:33.675246Z","iopub.status.idle":"2024-05-19T12:49:35.286789Z","shell.execute_reply.started":"2024-05-19T12:49:33.675206Z","shell.execute_reply":"2024-05-19T12:49:35.285732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix \n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nprint(confusion)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.290874Z","iopub.execute_input":"2024-05-19T12:49:35.292356Z","iopub.status.idle":"2024-05-19T12:49:35.366766Z","shell.execute_reply.started":"2024-05-19T12:49:35.292316Z","shell.execute_reply":"2024-05-19T12:49:35.360402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted     not_churn    churn\n# Actual\n# not_churn        2543      463\n# churn            692       1652  ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.369107Z","iopub.status.idle":"2024-05-19T12:49:35.370795Z","shell.execute_reply.started":"2024-05-19T12:49:35.370536Z","shell.execute_reply":"2024-05-19T12:49:35.370562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the overall accuracy\n\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.373908Z","iopub.status.idle":"2024-05-19T12:49:35.375419Z","shell.execute_reply.started":"2024-05-19T12:49:35.375170Z","shell.execute_reply":"2024-05-19T12:49:35.375196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's evaluate the other metrics as well\n\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.379403Z","iopub.status.idle":"2024-05-19T12:49:35.380908Z","shell.execute_reply.started":"2024-05-19T12:49:35.380668Z","shell.execute_reply":"2024-05-19T12:49:35.380691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the sensitivity\n\nTP/(TP+FN)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.383751Z","iopub.status.idle":"2024-05-19T12:49:35.385223Z","shell.execute_reply.started":"2024-05-19T12:49:35.384973Z","shell.execute_reply":"2024-05-19T12:49:35.384996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the specificity\n\nTN/(TN+FP)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.388127Z","iopub.status.idle":"2024-05-19T12:49:35.389517Z","shell.execute_reply.started":"2024-05-19T12:49:35.389286Z","shell.execute_reply":"2024-05-19T12:49:35.389307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding the Optimal Cutoff\n\nNow 0.5 was just arbitrary to loosely check the model performace. But in order to get good results, you need to optimise the threshold. So first let's plot an ROC curve to see what AUC we get.","metadata":{}},{"cell_type":"code","source":"# ROC function\n\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.392376Z","iopub.status.idle":"2024-05-19T12:49:35.393949Z","shell.execute_reply.started":"2024-05-19T12:49:35.393701Z","shell.execute_reply":"2024-05-19T12:49:35.393724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.397036Z","iopub.status.idle":"2024-05-19T12:49:35.398496Z","shell.execute_reply.started":"2024-05-19T12:49:35.398263Z","shell.execute_reply":"2024-05-19T12:49:35.398285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import matplotlib to plot the ROC curve\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.401089Z","iopub.status.idle":"2024-05-19T12:49:35.404129Z","shell.execute_reply.started":"2024-05-19T12:49:35.403838Z","shell.execute_reply":"2024-05-19T12:49:35.403873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the ROC function\n\ndraw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.408082Z","iopub.status.idle":"2024-05-19T12:49:35.408891Z","shell.execute_reply.started":"2024-05-19T12:49:35.408662Z","shell.execute_reply":"2024-05-19T12:49:35.408682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The area under the curve of the ROC is 0.86 which is quite good. So we seem to have a good model. Let's also check the sensitivity and specificity tradeoff to find the optimal cutoff point.","metadata":{}},{"cell_type":"code","source":"# Let's create columns with different probability cutoffs \n\nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.410106Z","iopub.status.idle":"2024-05-19T12:49:35.410755Z","shell.execute_reply.started":"2024-05-19T12:49:35.410567Z","shell.execute_reply":"2024-05-19T12:49:35.410585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a dataframe to see the values of accuracy, sensitivity, and specificity at different values of probabiity cutoffs\n\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.416055Z","iopub.status.idle":"2024-05-19T12:49:35.416855Z","shell.execute_reply.started":"2024-05-19T12:49:35.416629Z","shell.execute_reply":"2024-05-19T12:49:35.416650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot it as well\n\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.418057Z","iopub.status.idle":"2024-05-19T12:49:35.418692Z","shell.execute_reply.started":"2024-05-19T12:49:35.418508Z","shell.execute_reply":"2024-05-19T12:49:35.418525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see that around 0.42, you get the optimal values of the three metrics. So let's choose 0.42 as our cutoff now.","metadata":{}},{"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.42 else 0)\n\ny_train_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.419910Z","iopub.status.idle":"2024-05-19T12:49:35.420531Z","shell.execute_reply.started":"2024-05-19T12:49:35.420348Z","shell.execute_reply":"2024-05-19T12:49:35.420366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the accuracy now\n\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.427570Z","iopub.status.idle":"2024-05-19T12:49:35.428418Z","shell.execute_reply.started":"2024-05-19T12:49:35.428183Z","shell.execute_reply":"2024-05-19T12:49:35.428207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create the confusion matrix once again\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.429636Z","iopub.status.idle":"2024-05-19T12:49:35.430300Z","shell.execute_reply.started":"2024-05-19T12:49:35.430089Z","shell.execute_reply":"2024-05-19T12:49:35.430107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's evaluate the other metrics as well\n\nTP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.431505Z","iopub.status.idle":"2024-05-19T12:49:35.432138Z","shell.execute_reply.started":"2024-05-19T12:49:35.431949Z","shell.execute_reply":"2024-05-19T12:49:35.431966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Sensitivity\n\nTP/(TP+FN)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.433256Z","iopub.status.idle":"2024-05-19T12:49:35.433873Z","shell.execute_reply.started":"2024-05-19T12:49:35.433680Z","shell.execute_reply":"2024-05-19T12:49:35.433697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Specificity\n\nTN/(TN+FP)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.435004Z","iopub.status.idle":"2024-05-19T12:49:35.435635Z","shell.execute_reply.started":"2024-05-19T12:49:35.435454Z","shell.execute_reply":"2024-05-19T12:49:35.435472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cutoff point seems good to go!","metadata":{}},{"cell_type":"markdown","source":"## Making Predictions on the Test Set\n\nLet's now make predicitons on the test set.","metadata":{}},{"cell_type":"code","source":"# Scale the test set as well using just 'transform'\n\nX_test[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']] = scaler.transform(X_test[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.438796Z","iopub.status.idle":"2024-05-19T12:49:35.439506Z","shell.execute_reply.started":"2024-05-19T12:49:35.439302Z","shell.execute_reply":"2024-05-19T12:49:35.439322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the columns in X_train for X_test as well\n\nX_test = X_test[col]\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.440697Z","iopub.status.idle":"2024-05-19T12:49:35.442117Z","shell.execute_reply.started":"2024-05-19T12:49:35.441904Z","shell.execute_reply":"2024-05-19T12:49:35.441924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a constant to X_test\n\nX_test_sm = sm.add_constant(X_test[col])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.445017Z","iopub.status.idle":"2024-05-19T12:49:35.445750Z","shell.execute_reply.started":"2024-05-19T12:49:35.445543Z","shell.execute_reply":"2024-05-19T12:49:35.445564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check X_test_sm\n\nX_test_sm","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.450395Z","iopub.status.idle":"2024-05-19T12:49:35.451176Z","shell.execute_reply.started":"2024-05-19T12:49:35.450946Z","shell.execute_reply":"2024-05-19T12:49:35.450966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the required columns from X_test as well\n\nX_test.drop(['Lead Source_Reference', 'What is your current occupation_Housewife', \n             'What is your current occupation_Working Professional', 'Last Notable Activity_Had a Phone Conversation'], 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.452476Z","iopub.status.idle":"2024-05-19T12:49:35.453208Z","shell.execute_reply.started":"2024-05-19T12:49:35.452992Z","shell.execute_reply":"2024-05-19T12:49:35.453011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set and store it in the variable 'y_test_pred'\n\ny_test_pred = res.predict(sm.add_constant(X_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.454801Z","iopub.status.idle":"2024-05-19T12:49:35.456903Z","shell.execute_reply.started":"2024-05-19T12:49:35.456667Z","shell.execute_reply":"2024-05-19T12:49:35.456689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.465414Z","iopub.status.idle":"2024-05-19T12:49:35.466235Z","shell.execute_reply.started":"2024-05-19T12:49:35.465990Z","shell.execute_reply":"2024-05-19T12:49:35.466011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_pred to a dataframe\n\ny_pred_1 = pd.DataFrame(y_test_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.467478Z","iopub.status.idle":"2024-05-19T12:49:35.468130Z","shell.execute_reply.started":"2024-05-19T12:49:35.467940Z","shell.execute_reply":"2024-05-19T12:49:35.467958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the head\n\ny_pred_1.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.469456Z","iopub.status.idle":"2024-05-19T12:49:35.470186Z","shell.execute_reply.started":"2024-05-19T12:49:35.469974Z","shell.execute_reply":"2024-05-19T12:49:35.469992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_test to dataframe\n\ny_test_df = pd.DataFrame(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.471472Z","iopub.status.idle":"2024-05-19T12:49:35.472190Z","shell.execute_reply.started":"2024-05-19T12:49:35.471976Z","shell.execute_reply":"2024-05-19T12:49:35.471995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove index for both dataframes to append them side by side \n\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.473476Z","iopub.status.idle":"2024-05-19T12:49:35.474193Z","shell.execute_reply.started":"2024-05-19T12:49:35.473982Z","shell.execute_reply":"2024-05-19T12:49:35.474000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Append y_test_df and y_pred_1\n\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.476815Z","iopub.status.idle":"2024-05-19T12:49:35.477624Z","shell.execute_reply.started":"2024-05-19T12:49:35.477413Z","shell.execute_reply":"2024-05-19T12:49:35.477435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check 'y_pred_final'\n\ny_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.479582Z","iopub.status.idle":"2024-05-19T12:49:35.481994Z","shell.execute_reply.started":"2024-05-19T12:49:35.481762Z","shell.execute_reply":"2024-05-19T12:49:35.481783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename the column \n\ny_pred_final= y_pred_final.rename(columns = {0 : 'Conversion_Prob'})","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.489945Z","iopub.status.idle":"2024-05-19T12:49:35.490914Z","shell.execute_reply.started":"2024-05-19T12:49:35.490652Z","shell.execute_reply":"2024-05-19T12:49:35.490679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the head of y_pred_final\n\ny_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.492352Z","iopub.status.idle":"2024-05-19T12:49:35.493018Z","shell.execute_reply.started":"2024-05-19T12:49:35.492821Z","shell.execute_reply":"2024-05-19T12:49:35.492840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set using 0.45 as the cutoff\n\ny_pred_final['final_predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.42 else 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.494291Z","iopub.status.idle":"2024-05-19T12:49:35.495001Z","shell.execute_reply.started":"2024-05-19T12:49:35.494806Z","shell.execute_reply":"2024-05-19T12:49:35.494826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check y_pred_final\n\ny_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.496299Z","iopub.status.idle":"2024-05-19T12:49:35.496984Z","shell.execute_reply.started":"2024-05-19T12:49:35.496791Z","shell.execute_reply":"2024-05-19T12:49:35.496809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the overall accuracy\n\nmetrics.accuracy_score(y_pred_final['Converted'], y_pred_final.final_predicted)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.498522Z","iopub.status.idle":"2024-05-19T12:49:35.499608Z","shell.execute_reply.started":"2024-05-19T12:49:35.499205Z","shell.execute_reply":"2024-05-19T12:49:35.499224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final['Converted'], y_pred_final.final_predicted )\nconfusion2","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.506116Z","iopub.status.idle":"2024-05-19T12:49:35.506911Z","shell.execute_reply.started":"2024-05-19T12:49:35.506697Z","shell.execute_reply":"2024-05-19T12:49:35.506718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.508187Z","iopub.status.idle":"2024-05-19T12:49:35.508941Z","shell.execute_reply.started":"2024-05-19T12:49:35.508739Z","shell.execute_reply":"2024-05-19T12:49:35.508759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate sensitivity\nTP / float(TP+FN)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.510319Z","iopub.status.idle":"2024-05-19T12:49:35.511082Z","shell.execute_reply.started":"2024-05-19T12:49:35.510867Z","shell.execute_reply":"2024-05-19T12:49:35.510886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate specificity\nTN / float(TN+FP)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.512418Z","iopub.status.idle":"2024-05-19T12:49:35.515626Z","shell.execute_reply.started":"2024-05-19T12:49:35.515365Z","shell.execute_reply":"2024-05-19T12:49:35.515390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Precision-Recall View\n\nLet's now also build the training model using the precision-recall view","metadata":{}},{"cell_type":"code","source":"#Looking at the confusion matrix again\n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nconfusion","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.517109Z","iopub.status.idle":"2024-05-19T12:49:35.524621Z","shell.execute_reply.started":"2024-05-19T12:49:35.524326Z","shell.execute_reply":"2024-05-19T12:49:35.524354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Precision\nTP / TP + FP","metadata":{}},{"cell_type":"code","source":"confusion[1,1]/(confusion[0,1]+confusion[1,1])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.526064Z","iopub.status.idle":"2024-05-19T12:49:35.526848Z","shell.execute_reply.started":"2024-05-19T12:49:35.526625Z","shell.execute_reply":"2024-05-19T12:49:35.526645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Recall\nTP / TP + FN","metadata":{}},{"cell_type":"code","source":"confusion[1,1]/(confusion[1,0]+confusion[1,1])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.528230Z","iopub.status.idle":"2024-05-19T12:49:35.528925Z","shell.execute_reply.started":"2024-05-19T12:49:35.528725Z","shell.execute_reply":"2024-05-19T12:49:35.528744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Precision and recall tradeoff","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.530358Z","iopub.status.idle":"2024-05-19T12:49:35.531093Z","shell.execute_reply.started":"2024-05-19T12:49:35.530892Z","shell.execute_reply":"2024-05-19T12:49:35.530912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_final.Converted, y_train_pred_final.Predicted","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.532449Z","iopub.status.idle":"2024-05-19T12:49:35.533190Z","shell.execute_reply.started":"2024-05-19T12:49:35.532969Z","shell.execute_reply":"2024-05-19T12:49:35.532988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.534486Z","iopub.status.idle":"2024-05-19T12:49:35.537358Z","shell.execute_reply.started":"2024-05-19T12:49:35.537073Z","shell.execute_reply":"2024-05-19T12:49:35.537094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.545788Z","iopub.status.idle":"2024-05-19T12:49:35.546679Z","shell.execute_reply.started":"2024-05-19T12:49:35.546443Z","shell.execute_reply":"2024-05-19T12:49:35.546465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.44 else 0)\n\ny_train_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.548040Z","iopub.status.idle":"2024-05-19T12:49:35.548743Z","shell.execute_reply.started":"2024-05-19T12:49:35.548546Z","shell.execute_reply":"2024-05-19T12:49:35.548565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the accuracy now\n\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.549988Z","iopub.status.idle":"2024-05-19T12:49:35.550718Z","shell.execute_reply.started":"2024-05-19T12:49:35.550515Z","shell.execute_reply":"2024-05-19T12:49:35.550534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create the confusion matrix once again\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.552118Z","iopub.status.idle":"2024-05-19T12:49:35.552923Z","shell.execute_reply.started":"2024-05-19T12:49:35.552671Z","shell.execute_reply":"2024-05-19T12:49:35.552690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's evaluate the other metrics as well\n\nTP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.554265Z","iopub.status.idle":"2024-05-19T12:49:35.554980Z","shell.execute_reply.started":"2024-05-19T12:49:35.554782Z","shell.execute_reply":"2024-05-19T12:49:35.554800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Precision\n\nTP/(TP+FP)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.556291Z","iopub.status.idle":"2024-05-19T12:49:35.557828Z","shell.execute_reply.started":"2024-05-19T12:49:35.557608Z","shell.execute_reply":"2024-05-19T12:49:35.557629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Recall\n\nTP/(TP+FN)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.561639Z","iopub.status.idle":"2024-05-19T12:49:35.562378Z","shell.execute_reply.started":"2024-05-19T12:49:35.562144Z","shell.execute_reply":"2024-05-19T12:49:35.562182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cutoff point seems good to go!","metadata":{}},{"cell_type":"markdown","source":"## Making Predictions on the Test Set\n\nLet's now make predicitons on the test set.","metadata":{}},{"cell_type":"code","source":"# Make predictions on the test set and store it in the variable 'y_test_pred'\n\ny_test_pred = res.predict(sm.add_constant(X_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.574752Z","iopub.status.idle":"2024-05-19T12:49:35.575666Z","shell.execute_reply.started":"2024-05-19T12:49:35.575422Z","shell.execute_reply":"2024-05-19T12:49:35.575446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.577490Z","iopub.status.idle":"2024-05-19T12:49:35.579910Z","shell.execute_reply.started":"2024-05-19T12:49:35.579645Z","shell.execute_reply":"2024-05-19T12:49:35.579669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_pred to a dataframe\n\ny_pred_1 = pd.DataFrame(y_test_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.588393Z","iopub.status.idle":"2024-05-19T12:49:35.589198Z","shell.execute_reply.started":"2024-05-19T12:49:35.588961Z","shell.execute_reply":"2024-05-19T12:49:35.588983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the head\n\ny_pred_1.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.590441Z","iopub.status.idle":"2024-05-19T12:49:35.591110Z","shell.execute_reply.started":"2024-05-19T12:49:35.590912Z","shell.execute_reply":"2024-05-19T12:49:35.590930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_test to dataframe\n\ny_test_df = pd.DataFrame(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.592361Z","iopub.status.idle":"2024-05-19T12:49:35.593070Z","shell.execute_reply.started":"2024-05-19T12:49:35.592866Z","shell.execute_reply":"2024-05-19T12:49:35.592885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove index for both dataframes to append them side by side \n\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.594439Z","iopub.status.idle":"2024-05-19T12:49:35.595188Z","shell.execute_reply.started":"2024-05-19T12:49:35.594969Z","shell.execute_reply":"2024-05-19T12:49:35.594988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Append y_test_df and y_pred_1\n\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.596444Z","iopub.status.idle":"2024-05-19T12:49:35.597210Z","shell.execute_reply.started":"2024-05-19T12:49:35.596976Z","shell.execute_reply":"2024-05-19T12:49:35.596995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check 'y_pred_final'\n\ny_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.598467Z","iopub.status.idle":"2024-05-19T12:49:35.602113Z","shell.execute_reply.started":"2024-05-19T12:49:35.601845Z","shell.execute_reply":"2024-05-19T12:49:35.601871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename the column \n\ny_pred_final= y_pred_final.rename(columns = {0 : 'Conversion_Prob'})","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.605744Z","iopub.execute_input":"2024-05-19T12:49:35.610801Z","iopub.status.idle":"2024-05-19T12:49:35.663897Z","shell.execute_reply.started":"2024-05-19T12:49:35.610758Z","shell.execute_reply":"2024-05-19T12:49:35.662414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the head of y_pred_final\n\ny_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.665128Z","iopub.status.idle":"2024-05-19T12:49:35.665833Z","shell.execute_reply.started":"2024-05-19T12:49:35.665620Z","shell.execute_reply":"2024-05-19T12:49:35.665640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set using 0.44 as the cutoff\n\ny_pred_final['final_predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.44 else 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.667097Z","iopub.status.idle":"2024-05-19T12:49:35.667824Z","shell.execute_reply.started":"2024-05-19T12:49:35.667610Z","shell.execute_reply":"2024-05-19T12:49:35.667629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check y_pred_final\n\ny_pred_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.676106Z","iopub.status.idle":"2024-05-19T12:49:35.676913Z","shell.execute_reply.started":"2024-05-19T12:49:35.676681Z","shell.execute_reply":"2024-05-19T12:49:35.676702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the overall accuracy\n\nmetrics.accuracy_score(y_pred_final['Converted'], y_pred_final.final_predicted)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.683371Z","iopub.execute_input":"2024-05-19T12:49:35.683682Z","iopub.status.idle":"2024-05-19T12:49:35.725139Z","shell.execute_reply.started":"2024-05-19T12:49:35.683658Z","shell.execute_reply":"2024-05-19T12:49:35.722422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final['Converted'], y_pred_final.final_predicted )\nconfusion2","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.727327Z","iopub.status.idle":"2024-05-19T12:49:35.728819Z","shell.execute_reply.started":"2024-05-19T12:49:35.728588Z","shell.execute_reply":"2024-05-19T12:49:35.728610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.739767Z","iopub.execute_input":"2024-05-19T12:49:35.741348Z","iopub.status.idle":"2024-05-19T12:49:35.772718Z","shell.execute_reply.started":"2024-05-19T12:49:35.741309Z","shell.execute_reply":"2024-05-19T12:49:35.770221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Precision\n\nTP/(TP+FP)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.774757Z","iopub.status.idle":"2024-05-19T12:49:35.776301Z","shell.execute_reply.started":"2024-05-19T12:49:35.776040Z","shell.execute_reply":"2024-05-19T12:49:35.776063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Recall\n\nTP/(TP+FN)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:49:35.792944Z","iopub.execute_input":"2024-05-19T12:49:35.794586Z","iopub.status.idle":"2024-05-19T12:49:35.823751Z","shell.execute_reply.started":"2024-05-19T12:49:35.794549Z","shell.execute_reply":"2024-05-19T12:49:35.821259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"markdown","source":"There are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc.) in order to get a higher lead conversion.\nFirst, sort out the best prospects from the leads you have generated. 'TotalVisits' , 'Total Time Spent on Website' , 'Page Views Per Visit' which contribute most towards the probability of a lead getting converted.\nThen, You must keep a list of leads handy so that you can inform them about new courses, services, job offers and future higher studies. Monitor each lead carefully so that you can tailor the information you send to them. Carefully provide job offerings, information or courses that suits best according to the interest of the leads. A proper plan to chart the needs of each lead will go a long way to capture the leads as prospects.\nFocus on converted leads. Hold question-answer sessions with leads to extract the right information you need about them. Make further inquiries and appointments with the leads to determine their intention and mentality to join online courses. \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}